{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2e3077ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import *\n",
    "from tkinter import messagebox\n",
    "from tkinter import ttk\n",
    "import time\n",
    "import requests\n",
    "import re\n",
    "import math\n",
    "import sqlite3\n",
    "from multiprocessing import Process\n",
    "# НАШИ ПАРСЕРЫ\n",
    "import parser_gulliver\n",
    "import parser_crockid\n",
    "import parser_gloria\n",
    "import parser_acoola\n",
    "#\n",
    "from multiprocessing import Pool\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "import os\n",
    "import urllib.request\n",
    "import re\n",
    "import pandas as pd\n",
    "import datetime\n",
    "headers={\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.5112.81 Safari/537.36 Edg/104.0.1293.54\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8be5a49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Interface():\n",
    "    def callbackFunc(event):\n",
    "        global urlbinance\n",
    "        url1 = comboExample.get()\n",
    "        if(url1 == \"Gulliver\"):  \n",
    "            gulliver()\n",
    "            messagebox.showinfo(\"Выполнено!\")\n",
    "        if(url1 == \"Gloria\"):  \n",
    "            gloria()\n",
    "            messagebox.showinfo(\"Выполнено!\")\n",
    "        if(url1 == \"Sela\"):  \n",
    "            sela()\n",
    "            messagebox.showinfo(\"Выполнено!\")\n",
    "        if(url1 == \"Crockid\"):  \n",
    "            crockid()\n",
    "            messagebox.showinfo(\"Выполнено!\")\n",
    "        if(url1 == \"Acoola\"):  \n",
    "            acoola()\n",
    "            messagebox.showinfo(\"Выполнено!\")\n",
    "    pars = tk.Tk()\n",
    "    pars.geometry('300x300')\n",
    "    pars.title('Программа для парсинга')\n",
    "    title = Label(pars, text='Программа для арбитража', font='Arial 40')\n",
    "\n",
    "    labelTop = tk.Label(pars,\n",
    "                text = \"Какой сайт хотите спарсить?\")\n",
    "    labelTop.pack()\n",
    "    comboExample = ttk.Combobox(pars, \n",
    "                        values=[\n",
    "                                \"Gulliver\", \n",
    "                                \"Gloria\",\n",
    "                                \"Sela\",\n",
    "                                \"Crockid\",\n",
    "                                \"Acoola\"],state=\"readonly\")\n",
    "    comboExample.pack()\n",
    "    comboExample.bind(\"<<ComboboxSelected>>\", callbackFunc)\n",
    "    pars.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df45d060",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gulliver():\n",
    "    link = \"https://www.gulliver.ru/catalog/odezhda/b/gulliver?sort=our_choice,asc\"\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    dec2 = soup.find_all('a',{'class':\"page-pagination__link\"})[-1:]\n",
    "    dictionary = {}\n",
    "    for item in dec2:\n",
    "          amount_pages = item.text\n",
    "    pool = Pool()\n",
    "    pool.map(parser_gulliver.parser, range(int(amount_pages)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85023ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gloria():\n",
    "#ДЕТИ\n",
    "    link = \"https://www.gloria-jeans.ru/c/kids\"\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    dec2 = soup.find_all('a',{'class':\"uk-pagination__number js-pagination-link\"})[-1:]\n",
    "    for item in dec2:\n",
    "          amount_pages=item.text\n",
    "    pool = Pool()\n",
    "    pool.map(parser_gloria.parser, range(int(amount_pages)))\n",
    "#ПОДРОСТКИ\n",
    "    link = \"https://www.gloria-jeans.ru/c/teenagers\"\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    dec2 = soup.find_all('a',{'class':\"uk-pagination__number js-pagination-link\"})[-1:]\n",
    "    for item in dec2:\n",
    "          amount_pages=item.text\n",
    "    pool = Pool()\n",
    "    pool.map(parser_gloria.parser2, range(int(amount_pages)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1acf975d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crockid():\n",
    "#МАЛЬЧИКИ\n",
    "    link = \"https://www.crockid.ru/catalog/odezhda-dlya-malchikov\"\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    dec = soup.find_all('div',{'class':\"paginateBlock\"})\n",
    "    dec_mas = []\n",
    "    for item in dec:\n",
    "        dec_mas.append(item.text.strip())\n",
    "    amount_pages=re.findall(r'\\.(\\d+)\\s',dec_mas[0])\n",
    "    pool = Pool()\n",
    "    pool.map(parser_crockid.parser, range(int(amount_pages[0]))) \n",
    "#ДЕВОЧКИ\n",
    "    link = \"https://www.crockid.ru/catalog/girls\"\n",
    "    amount_pages=[]\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    dec = soup.find_all('div',{'class':\"paginateBlock\"})\n",
    "    dec_mas = []\n",
    "    for item in dec:\n",
    "        dec_mas.append(item.text.strip())\n",
    "    amount_pages=re.findall(r'\\.(\\d+)\\s',dec_mas[0])\n",
    "    pool = Pool()\n",
    "    pool.map(parser_crockid.parser2, range(int(amount_pages[0])))\n",
    "#НОВОРОЖДЕННЫЕ\n",
    "    link = \"https://www.crockid.ru/catalog/baby\"\n",
    "    amount_pages=[]\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    dec = soup.find_all('div',{'class':\"paginateBlock\"})\n",
    "    dec_mas = []\n",
    "    for item in dec:\n",
    "        dec_mas.append(item.text.strip())\n",
    "    amount_pages=re.findall(r'\\.(\\d+)\\s',dec_mas[0])\n",
    "    pool = Pool()\n",
    "    pool.map(parser_crockid.parser3, range(int(amount_pages[0]))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9154d75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser_sela(conn, driver, n): \n",
    "    print(n)\n",
    "    now = datetime.datetime.now()\n",
    "    driver.get(\"https://www.sela.ru/eshop/kids/girl/?page=\" + str(n+1) + \"&price_min=199&price_max=6999&sort=publish_datetime&order=desc&_=1660812788266\")\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    links = soup.find_all('li',{'class':\"product-catalog__item js-product-catalog-item\"})\n",
    "    links_mas = []\n",
    "#ПОЛУЧАЕМ ССЫЛКИ\n",
    "    for item in links:\n",
    "        links_mas.append(item.find('a')['href'])\n",
    "#ПОЛУЧАЕМ КАРТИНКИ\n",
    "    images_mas = []\n",
    "    for item in soup.find_all('li',{'class':\"product-card__slider-item js-product-card-slide is-active\"}):\n",
    "        if item.find('img').get('src')!=None:\n",
    "            images_mas.append(item.find('img').get('src'))\n",
    "    k=0\n",
    "#ИДЕМ ПО ЦИКЛУ\n",
    "    for item in links_mas:\n",
    "        dictionary = {}\n",
    "        link = \"https://www.sela.ru\" + item\n",
    "        dictionary['Ссылка'] = link\n",
    "        dictionary['Ссылка_картинка'] = images_mas[k]\n",
    "        dictionary['Дата']=now.strftime(\"%d-%m-%Y\")\n",
    "        k+=1\n",
    "        try:\n",
    "            driver.get(link)\n",
    "        except TimeoutException:\n",
    "            driver.get(link)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "#ЦЕНЫ\n",
    "        prices = soup.find_all('div',{'class':\"product-page__price\"})\n",
    "        price_mas=\"\"\n",
    "        price_mas_sale=[]\n",
    "        for item in prices:\n",
    "            price_mas+=item.text.strip().replace(\"\\n\",\"|\")\n",
    "        if \"|\" in price_mas:\n",
    "            price_mas_sale=re.split(r'\\|',price_mas)\n",
    "            dictionary['Цена'] = price_mas_sale[1]\n",
    "            dictionary['Цена_со_скидкой'] = price_mas_sale[0]\n",
    "        else:\n",
    "            dictionary['Цена']=price_mas\n",
    "            dictionary['Цена_со_скидкой']=\"-\"\n",
    "#НАЗВАНИЕ\n",
    "        name = soup.find_all('h1',{'class':\"product-page__title\"})\n",
    "        for item in name:\n",
    "            dictionary['Наименование'] = item.text.strip()\n",
    "#БЛОК ДЕТАЛЕЙ\n",
    "        details_soup = soup.find_all('div',{'style':\"margin-top:15px\"})\n",
    "        details = []\n",
    "        for item in details_soup:\n",
    "            details.append(item.text.strip())\n",
    "        if len(details)>=3:\n",
    "            if \"Возраст:\" in details[0]:\n",
    "                dictionary['Возраст']=details[0][details[0].index(\":\")+2:]\n",
    "            if \"Сезон:\" in details[1]:\n",
    "                dictionary['Сезон']=details[1][details[1].index(\":\")+2:]\n",
    "            if \"Состав:\" in details[2]:\n",
    "                dictionary['Состав']=details[2][details[2].index(\":\")+2:]\n",
    "        else:\n",
    "            if \"Сезон:\" in details[0]:\n",
    "                dictionary['Сезон']=details[0][details[0].index(\":\")+2:]\n",
    "            if \"Состав:\" in details[1]:\n",
    "                dictionary['Состав']=details[1][details[1].index(\":\")+2:]\n",
    "#АРТИКУЛ\n",
    "        art = soup.find_all('b',{'style':\"white-space:nowrap;\"})\n",
    "        for item in art:\n",
    "             dictionary['Артикул']=item.text.strip()\n",
    "#БЛОК РАЗМЕРОВ\n",
    "        size = []\n",
    "        size_s = \"\"\n",
    "        size_ros = \"\"\n",
    "        available = []\n",
    "        av = soup.find_all('span', {'data-omni':\"false\"})\n",
    "        for item in av:\n",
    "            if item.find('span', {'class':\"stock-high\"})!=None:\n",
    "                available.append(item.find('span', {'class':\"stock-high\"}).text.strip().replace(\"\\xa0\",\" \"))\n",
    "                size.append(re.split(r\", \",item['data-sku_name'])[1])\n",
    "            if item.find('span', {'class':\"stock-low\"})!=None:\n",
    "                available.append(item.find('span', {'class':\"stock-low\"}).text.strip().replace(\"\\xa0\",\" \"))\n",
    "                size.append(re.split(r\", \",item['data-sku_name'])[1])\n",
    "            if item.find('span', {'class':\"stock-critical\"})!=None:\n",
    "                available.append(item.find('span', {'class':\"stock-critical\"}).text.strip().replace(\"\\xa0\",\" \"))\n",
    "                size.append(re.split(r\", \",item['data-sku_name'])[1])\n",
    "        av = soup.find_all('span', {'data-omni':\"true\"})\n",
    "        for item in av:\n",
    "            if item.find('span', {'class':\"stock-high\"})!=None:\n",
    "                available.append(item.find('span', {'class':\"stock-high\"}).text.strip().replace(\"\\xa0\",\" \"))\n",
    "                size.append(re.split(r\", \",item['data-sku_name'])[1])\n",
    "            if item.find('span', {'class':\"stock-low\"})!=None:\n",
    "                available.append(item.find('span', {'class':\"stock-low\"}).text.strip().replace(\"\\xa0\",\" \"))\n",
    "                size.append(re.split(r\", \",item['data-sku_name'])[1])\n",
    "            if item.find('span', {'class':\"stock-critical\"})!=None:\n",
    "                available.append(item.find('span', {'class':\"stock-critical\"}).text.strip().replace(\"\\xa0\",\" \"))\n",
    "                size.append(re.split(r\", \",item['data-sku_name'])[1])\n",
    "        size_active = soup.find_all('li', {'class':\"product-page__size-item js-size-item is-active-size\"})\n",
    "        for item in size_active:\n",
    "            size_ac = item.text.strip()\n",
    "        if size_ac not in size:\n",
    "            available.append(\"сообщить о поступлении\")\n",
    "            size.append(size_ac)\n",
    "        for i in range(len(available)):\n",
    "            if available[i]=='сообщить о поступлении' or available[i]=='в наличии только в магазинах':\n",
    "                size_ros+=size[i] + \" \"\n",
    "            else:\n",
    "                size_s+=size[i] + \" \"\n",
    "        dictionary['Размер']=size_s\n",
    "        dictionary['Размер_розница']=size_ros\n",
    "#ПОЛ/ТИП\n",
    "        size_active = soup.find_all('span',{'itemprop':\"name\"})\n",
    "        if len(size_active)>=3:\n",
    "            for item in size_active[0]:\n",
    "                dictionary['Товарная_группа1'] = item.text.strip()\n",
    "            for item in size_active[1]:\n",
    "                dictionary['Товарная_группа2'] = item.text.strip()\n",
    "            for item in size_active[2]:\n",
    "                dictionary['Товарная_группа3'] = item.text.strip()\n",
    "        elif len(size_active)==2:\n",
    "            for item in size_active[0]:\n",
    "                dictionary['Товарная_группа1'] = item.text.strip()\n",
    "            for item in size_active[1]:\n",
    "                dictionary['Товарная_группа2'] = item.text.strip()\n",
    "        else:\n",
    "            for item in size_active[0]:\n",
    "                dictionary['Товарная_группа1'] = item.text.strip()\n",
    "#ЦВЕТ\n",
    "        colour = soup.find_all('li',{'class':\"product-page__color-item is-border is-active-color\"})\n",
    "        if colour == []:\n",
    "            colour = soup.find_all('li',{'class':\"product-page__color-item is-active-color\"})\n",
    "        for item in colour:\n",
    "            dictionary['Цвет']=item['title']\n",
    "#ОПИСАНИЕ\n",
    "        description = soup.find_all('div', {'class':\"product-page__item-text js-content\"})[0]\n",
    "        desc_mas = []\n",
    "        for item in description:\n",
    "             desc_mas.append(item.text.strip())\n",
    "        dictionary['Описание']=desc_mas[2]\n",
    "#ЗАБИВАЕМ В SQL\n",
    "        if dictionary['Ссылка'] in ultradata[0] and (dictionary['Цена']!=ultradata[1][ultradata[0].index(dictionary['Ссылка'])] or dictionary['Цена_со_скидкой']!=ultradata[2][ultradata[0].index(dictionary['Ссылка'])]):\n",
    "            df = pd.DataFrame([dictionary], columns= ['Наименование','Цена','Цена_со_скидкой','Категория','Возраст','Состав','Цвет','Сезон','Артикул','Описание','Товарная_группа1','Товарная_группа2','Товарная_группа3','Размер','Размер_розница','Ссылка','Ссылка_картинка', 'Дата'])\n",
    "            df.to_sql('sela', conn, if_exists='append', index = False)\n",
    "        elif dictionary['Ссылка'] not in ultradata[0]:\n",
    "            df = pd.DataFrame([dictionary], columns= ['Наименование','Цена','Цена_со_скидкой','Категория','Возраст','Состав','Цвет','Сезон','Артикул','Описание','Товарная_группа1','Товарная_группа2','Товарная_группа3','Размер','Размер_розница','Ссылка','Ссылка_картинка', 'Дата'])\n",
    "            df.to_sql('sela', conn, if_exists='append', index = False)\n",
    "def parser_sela2(conn, driver, n):\n",
    "    print(n)\n",
    "    now = datetime.datetime.now()\n",
    "    driver.get(\"https://www.sela.ru/eshop/kids/boy/?page=\" + str(n+1) + \"&price_min=49&price_max=6999&age%5B0%5D=5068&age%5B1%5D=5067&sort=publish_datetime&order=desc\")\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    links = soup.find_all('li',{'class':\"product-card__slider-item js-product-card-slide is-active\"})\n",
    "    links_mas = []\n",
    "#ПОЛУЧАЕМ ССЫЛКИ\n",
    "    for item in links:\n",
    "        links_mas.append(item.find('a')['href'])\n",
    "#ПОЛУЧАЕМ КАРТИНКИ\n",
    "    images = soup.find_all('li',{'class':\"product-card__slider-item js-product-card-slide\"})\n",
    "    images_mas = []\n",
    "    for item in soup.find_all('li',{'class':\"product-card__slider-item js-product-card-slide\"}):\n",
    "        if item.find('img').get('src')!=None:\n",
    "            images_mas.append(item.find('img').get('src'))\n",
    "    k=0\n",
    "    for item in links_mas:\n",
    "        dictionary = {}\n",
    "        link = \"https://www.sela.ru\" + item\n",
    "        dictionary['Ссылка'] = link\n",
    "        dictionary['Ссылка_картинка'] = images_mas[k]\n",
    "        dictionary['Дата']=now.strftime(\"%d-%m-%Y\")\n",
    "        k+=1\n",
    "        try:\n",
    "            driver.get(link)\n",
    "        except TimeoutException:\n",
    "            driver.get(link)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "#ЦЕНЫ\n",
    "        prices = soup.find_all('div',{'class':\"product-page__price\"})\n",
    "        price_mas=\"\"\n",
    "        price_mas_sale=[]\n",
    "        for item in prices:\n",
    "            price_mas+=item.text.strip().replace(\"\\n\",\"|\")\n",
    "        if \"|\" in price_mas:\n",
    "            price_mas_sale=re.split(r'\\|',price_mas)\n",
    "            dictionary['Цена'] = price_mas_sale[1]\n",
    "            dictionary['Цена_со_скидкой'] = price_mas_sale[0]\n",
    "        else:\n",
    "            dictionary['Цена']=price_mas\n",
    "            dictionary['Цена_со_скидкой']=\"-\"\n",
    "#НАЗВАНИЕ\n",
    "        name = soup.find_all('h1',{'class':\"product-page__title\"})\n",
    "        for item in name:\n",
    "            dictionary['Наименование'] = item.text.strip()\n",
    "#БЛОК ДЕТАЛЕЙ\n",
    "        details_soup = soup.find_all('div',{'style':\"margin-top:15px\"})\n",
    "        details = []\n",
    "        for item in details_soup:\n",
    "            details.append(item.text.strip())\n",
    "        if len(details)>=3:\n",
    "            if \"Возраст:\" in details[0]:\n",
    "                dictionary['Возраст']=details[0][details[0].index(\":\")+2:]\n",
    "            if \"Сезон:\" in details[1]:\n",
    "                dictionary['Сезон']=details[1][details[1].index(\":\")+2:]\n",
    "            if \"Состав:\" in details[2]:\n",
    "                dictionary['Состав']=details[2][details[2].index(\":\")+2:]\n",
    "        else:\n",
    "            if \"Сезон:\" in details[0]:\n",
    "                dictionary['Сезон']=details[0][details[0].index(\":\")+2:]\n",
    "            if \"Состав:\" in details[1]:\n",
    "                dictionary['Состав']=details[1][details[1].index(\":\")+2:]\n",
    "#АРТИКУЛ\n",
    "        art = soup.find_all('b',{'style':\"white-space:nowrap;\"})\n",
    "        for item in art:\n",
    "             dictionary['Артикул']=item.text.strip()\n",
    "#РАЗМЕРЫ\n",
    "        size = []\n",
    "        size_s = \"\"\n",
    "        size_ros = \"\"\n",
    "        available = []\n",
    "        av = soup.find_all('span', {'data-omni':\"false\"})\n",
    "        for item in av:\n",
    "            if item.find('span', {'class':\"stock-high\"})!=None:\n",
    "                available.append(item.find('span', {'class':\"stock-high\"}).text.strip().replace(\"\\xa0\",\" \"))\n",
    "                size.append(re.split(r\", \",item['data-sku_name'])[1])\n",
    "            if item.find('span', {'class':\"stock-low\"})!=None:\n",
    "                available.append(item.find('span', {'class':\"stock-low\"}).text.strip().replace(\"\\xa0\",\" \"))\n",
    "                size.append(re.split(r\", \",item['data-sku_name'])[1])\n",
    "            if item.find('span', {'class':\"stock-critical\"})!=None:\n",
    "                available.append(item.find('span', {'class':\"stock-critical\"}).text.strip().replace(\"\\xa0\",\" \"))\n",
    "                size.append(re.split(r\", \",item['data-sku_name'])[1])\n",
    "        av = soup.find_all('span', {'data-omni':\"true\"})\n",
    "        for item in av:\n",
    "            if item.find('span', {'class':\"stock-high\"})!=None:\n",
    "                available.append(item.find('span', {'class':\"stock-high\"}).text.strip().replace(\"\\xa0\",\" \"))\n",
    "                size.append(re.split(r\", \",item['data-sku_name'])[1])\n",
    "            if item.find('span', {'class':\"stock-low\"})!=None:\n",
    "                available.append(item.find('span', {'class':\"stock-low\"}).text.strip().replace(\"\\xa0\",\" \"))\n",
    "                size.append(re.split(r\", \",item['data-sku_name'])[1])\n",
    "            if item.find('span', {'class':\"stock-critical\"})!=None:\n",
    "                available.append(item.find('span', {'class':\"stock-critical\"}).text.strip().replace(\"\\xa0\",\" \"))\n",
    "                size.append(re.split(r\", \",item['data-sku_name'])[1])\n",
    "        size_active = soup.find_all('li', {'class':\"product-page__size-item js-size-item is-active-size\"})\n",
    "        for item in size_active:\n",
    "            size_ac = item.text.strip()\n",
    "        if size_ac not in size:\n",
    "            available.append(\"сообщить о поступлении\")\n",
    "            size.append(size_ac)\n",
    "        for i in range(len(available)):\n",
    "            if available[i]=='сообщить о поступлении' or available[i]=='в наличии только в магазинах':\n",
    "                size_ros+=size[i] + \" \"\n",
    "            else:\n",
    "                size_s+=size[i] + \" \"\n",
    "        dictionary['Размер']=size_s\n",
    "        dictionary['Размер_розница']=size_ros\n",
    "#ПОЛ/ТИП\n",
    "        size_active = soup.find_all('span',{'itemprop':\"name\"})\n",
    "        if len(size_active)>=3:\n",
    "            for item in size_active[0]:\n",
    "                dictionary['Товарная_группа1'] = item.text.strip()\n",
    "            for item in size_active[1]:\n",
    "                dictionary['Товарная_группа2'] = item.text.strip()\n",
    "            for item in size_active[2]:\n",
    "                dictionary['Товарная_группа3'] = item.text.strip()\n",
    "        elif len(size_active)==2:\n",
    "            for item in size_active[0]:\n",
    "                dictionary['Товарная_группа1'] = item.text.strip()\n",
    "            for item in size_active[1]:\n",
    "                dictionary['Товарная_группа2'] = item.text.strip()\n",
    "        else:\n",
    "            for item in size_active[0]:\n",
    "                dictionary['Товарная_группа1'] = item.text.strip()\n",
    "#ЦВЕТ\n",
    "        colour = soup.find_all('li',{'class':\"product-page__color-item is-border is-active-color\"})\n",
    "        if colour == []:\n",
    "            colour = soup.find_all('li',{'class':\"product-page__color-item is-active-color\"})\n",
    "        for item in colour:\n",
    "            dictionary['Цвет']=item['title']\n",
    "#ОПИСАНИЕ\n",
    "        description = soup.find_all('div', {'class':\"product-page__item-text js-content\"})[0]\n",
    "        desc_mas = []\n",
    "        for item in description:\n",
    "             desc_mas.append(item.text.strip())\n",
    "        dictionary['Описание']=desc_mas[2]\n",
    "#ЗАБИВАЕМ В SQL\n",
    "        if dictionary['Ссылка'] in ultradata[0] and (dictionary['Цена']!=ultradata[1][ultradata[0].index(dictionary['Ссылка'])] or dictionary['Цена_со_скидкой']!=ultradata[2][ultradata[0].index(dictionary['Ссылка'])]):\n",
    "            df = pd.DataFrame([dictionary], columns= ['Наименование','Цена','Цена_со_скидкой','Категория','Возраст','Состав','Цвет','Сезон','Артикул','Описание','Товарная_группа1','Товарная_группа2','Товарная_группа3','Размер','Размер_розница','Ссылка','Ссылка_картинка', 'Дата'])\n",
    "            df.to_sql('sela', conn, if_exists='append', index = False)\n",
    "        elif dictionary['Ссылка'] not in ultradata[0]:\n",
    "            df = pd.DataFrame([dictionary], columns= ['Наименование','Цена','Цена_со_скидкой','Категория','Возраст','Состав','Цвет','Сезон','Артикул','Описание','Товарная_группа1','Товарная_группа2','Товарная_группа3','Размер','Размер_розница','Ссылка','Ссылка_картинка', 'Дата'])\n",
    "            df.to_sql('sela', conn, if_exists='append', index = False)\n",
    "def sela():\n",
    "    global ultradata\n",
    "    conn = sqlite3.connect('database3')\n",
    "    c = conn.cursor()\n",
    "    c.execute('CREATE TABLE IF NOT EXISTS sela (Наименование text, Цена number, Цена_со_скидкой number, Возраст text, Состав text, Категория text, Цвет text, Сезон text, Артикул text, Описание text, Товарная_группа1 text, Товарная_группа2 text, Товарная_группа3 text, Размер text, Размер_розница text, Ссылка text,Ссылка_картинка text, Дата text)')\n",
    "    conn.commit()\n",
    "    cursor = conn.execute(\"SELECT Ссылка, Цена, Цена_со_скидкой from sela\")\n",
    "    ultradata = [[] for i in range(3)]\n",
    "    for row in cursor:\n",
    "        ultradata[0].append(row[0])\n",
    "        ultradata[1].append(row[1])\n",
    "        ultradata[2].append(row[2])\n",
    "    conn.commit()\n",
    "#ЗАПУСКАЕМ ДРАЙВЕР\n",
    "    options = Options()\n",
    "    options.add_argument('--disable-site-isolation-trials')\n",
    "    options.w3c = True\n",
    "    driver = webdriver.Chrome(r'chromedriver.exe', options=options)\n",
    "#ДЕВОЧКИ\n",
    "    link = \"https://www.sela.ru/eshop/kids/girl/?price_min=199&price_max=6999&age%5B%5D=5068&age%5B%5D=5067&sort=publish_datetime&order=desc\"\n",
    "    driver.get(link)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "#КОЛИЧЕСТВО СТРАНИЦ\n",
    "    dec2 = soup.find_all('a',{'class':\"pagination-wrapper__pagination-list-item\"})[-1:]\n",
    "    for item in dec2:\n",
    "          amount_pages=item.text\n",
    "    for n in range(int(amount_pages)):\n",
    "        parser_sela(conn, driver, n)\n",
    "#МАЛЬЧИКИ\n",
    "    link = \"https://www.sela.ru/eshop/kids/boy/?price_min=49&price_max=6999&age%5B%5D=5068&age%5B%5D=5067&sort=publish_datetime&order=desc\"\n",
    "    driver.get(link)\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "#КОЛИЧЕСТВО СТРАНИЦ\n",
    "    dec2 = soup.find_all('a',{'class':\"pagination-wrapper__pagination-list-item\"})[-1:]\n",
    "    for item in dec2:\n",
    "          amount_pages=item.text\n",
    "    for n in range(int(amount_pages)):\n",
    "        parser_sela2(conn, driver,n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a3c5ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def acoola():\n",
    "#ДЕВОЧКИ 2-8\n",
    "    link = \"https://acoolakids.ru/vsya-odezhda-dlya-devochek-2-8\"\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    dec = soup.find_all('nav',{'class':\"ak-pagination\"})[-1]\n",
    "    for item in dec:\n",
    "        amount_pages=item.text.split('.')[-1]\n",
    "    pool = Pool()\n",
    "    pool.map(parser_acoola.parser, range(int(amount_pages)))    \n",
    "#ДЕВОЧКИ 8-14\n",
    "    link = \"https://acoolakids.ru/vsya-odezhda-dlya-devochek-8-14\"\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    dec = soup.find_all('nav',{'class':\"ak-pagination\"})[-1]\n",
    "    for item in dec:\n",
    "        amount_pages=item.text.split('.')[-1]\n",
    "    pool = Pool()\n",
    "    pool.map(parser_acoola.parser2, range(int(amount_pages)))    \n",
    "#МАЛЬЧИКИ 2-8\n",
    "    link = \"https://acoolakids.ru/vsya-odezhda-dlya-malchikov-3-8\"\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    dec = soup.find_all('nav',{'class':\"ak-pagination\"})[-1]\n",
    "    for item in dec:\n",
    "        amount_pages=item.text.split('.')[-1]\n",
    "    pool = Pool()\n",
    "    pool.map(parser_acoola.parser3, range(int(amount_pages)))    \n",
    "#ДЕВОЧКИ 8-14\n",
    "    link = \"https://acoolakids.ru/vsya-odezhda-dlya-malchikov-8-14\"\n",
    "    page = requests.get(link)\n",
    "    soup = BeautifulSoup(page.text, 'html.parser')\n",
    "    dec = soup.find_all('nav',{'class':\"ak-pagination\"})[-1]\n",
    "    for item in dec:\n",
    "        amount_pages=item.text.split('.')[-1]\n",
    "    pool = Pool()\n",
    "    pool.map(parser_acoola.parser4, range(int(amount_pages)))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6188cf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    Interface()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bfb8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ac19b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d779117a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83875735",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6cdee7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
